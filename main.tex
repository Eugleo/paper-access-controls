%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{float}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{xurl}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{packages/icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Configure cleveref names for appendix sections
\crefname{appendix}{Appendix}{Appendices}
\Crefname{appendix}{Appendix}{Appendices}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
  \icmltitle{Access Controls Will Solve the Dual-Use Dilemma}

  % It is OKAY to include author information, even for blind
  % submissions: the style file will automatically remove it for you
  % unless you've provided the [accepted] option to the icml2025
  % package.

  % List of affiliations: The first argument should be a (short)
  % identifier you will use later to specify author affiliations
  % Academic affiliations should list Department, University, City,
  % Region, Country
  % Industry affiliations should list Company, City, Region, Country

  % You can specify symbols, otherwise they are numbered in order.
  % Ideally, you should not use this facility. Affiliations will be numbered
  % in order of appearance and this is the preferred way.
  \icmlsetsymbol{equal}{*}

  \begin{icmlauthorlist}
    \icmlauthor{Evžen Wybitul}{eth}
  \end{icmlauthorlist}

  \icmlaffiliation{eth}{ETH Zurich, Switzerland}

  \icmlcorrespondingauthor{Evžen Wybitul}{wybitul.evzen@gmail.com}

  % You may provide any keywords that you
  % find helpful for describing your paper; these are used to populate
  % the "keywords" metadata in the PDF but will not be shown in the document
  \icmlkeywords{Gradient Routing, Modularization, AI Safety,
  Unlearning, Access Control, Technical AI Governance}

  \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the
% start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention
% equal contributiono
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
  AI safety systems face a dual-use dilemma.
  The same request can be either harmless or harmful depending on who made it and why.
  Thus, if the system makes decisions based solely on the request's content, it will refuse some legitimate queries and let harmful ones pass.
  To address this, we propose a conceptual access control framework, based on verified user credentials (such as institutional affiliation) and classifiers that assign model outputs to risk categories (such as advanced virology).
  The system permits responses only when the user's verified credentials match the category's requirements.
  For implementation of the model output classifiers, we introduce a theoretical approach utilizing small, gated expert modules integrated into the generator model, trained with gradient routing, that enable efficient risk detection without the capability gap problems of external monitors.
  While open questions remain about the verification mechanisms, risk categories, and the technical implementation, our framework makes the first step toward enabling granular governance of AI capabilities: verified users gain access to specialized knowledge without arbitrary restrictions, while adversaries are blocked from it.
  This contextual approach reconciles model utility with robust safety, addressing the dual-use dilemma.
\end{abstract}

\section{Introduction} \label{section:introduction}

% Safety systems face a fundamental tradeoff between preventing misuse and preserving utility for legitimate users.
% Current approaches cannot optimize this tradeoff because they lack trustworthy information about user context.
% When a legitimate researcher and a bioterrorist ask identical questions about viral mutations, content-based systems must give the same response to both.
% This constraint forces suboptimal decisions: systems either refuse both requests (sacrificing utility) or allow both (sacrificing safety).
% Any system that treats different user types identically for identical requests cannot reach the optimal safety-utility balance.

User requests --- and with them, model outputs --- exist on a
spectrum from clearly benign to clearly harmful, with most falling in
the grey zone in the middle (example in \cref{figure:main}). In the
grey zone, the same output could be considered harmful or harmless,
depending not on its content, but on its \emph{real-world context}:
who requested it and for what purpose.

% TODO Make this consistent with the aerosolisation examples: only use one.

\begin{figure}[t]
  \vskip 0.2in
  \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{assets/main_figure.pdf}}
    \caption{
      The user is asking a question from the grey zone: a question that could be either harmless or harmful, depending on its real-world context.
      The schema shows how the system we propose would handle it.
      (1)~The model is trained to be helpful and begins to answer the question.
      (2)~During the forward pass, the model activates its virology expert module because it is relevant to the question.
      (3)~The activation of the expert is observed by an external mechanism that immediately (4)~checks in the company's database if the user has the required authorization to access virology knowledge.
      (5)~Since they don't, the model is stopped.
    If they did, the model would be allowed to give an answer.}
    \label{figure:main}
  \end{center}
  \vskip -0.2in
\end{figure}

Safety systems that rely solely on content analysis immediately face
the \emph{dual-use dilemma}. Since the same request can be either
harmless or harmful depending on the context, wherever they draw the
refusal line, they will restrict model utility for legitimate users
while letting slip harmful requests from adversaries. Some safety
systems try to address this by considering real-world context
alongside content. However, they typically infer the context from the
content itself, making it easy for adversaries to fabricate.

In this paper, we argue that informative, hard-to-fabricate
real-world context could be obtained using user-level verifications
such as institutional affiliation, or know-your-customer checks. We
then address the dual-use dilemma with two contributions:
\begin{enumerate}
  \item We show how this type of context could be used jointly with content analysis in a safety system based on access controls \cite{butler1974}. First, generated content would be classified into risk categories. Then, a check would be performed to see whether the user has the verifications required to access the detected categories.
  \item We propose a novel technical approach to risk
    category classification that is based on gradient routing \cite{cloud2024gradientroutingmaskinggradients}. Our proposal avoids having the capability gap between a model and its monitors that can make output monitoring methods non-robust \cite{jin2024jailbreakinglargelanguagemodels}.
\end{enumerate}

Our framework is a first step toward solving the challenge of ``detection and authorization of dual-use capability at inference time'' that was highlighted by a recent survey of problems in technical AI governance \cite{reuel2025openproblemstechnicalai} and also raised by the \citet{NIST_AI_800_1_ipd_2024}.
As such, it has important governance implications, potentially enabling a more nuanced regulatory approach where access to powerful AI capabilities is stratified rather than binary, with policies that differentiate between user types and user contexts rather than focusing solely on model capabilities.
The choice of appropriate verification mechanisms and risk categories remains for future work and should ideally happen jointly with stakeholders from academia, AI governance, and industry.
Nevertheless, our approach offers a promising direction for addressing the dual-use dilemma.

\section{Current Safety Methods Don't Solve the Dual-Use Dilemma}
\label{section:current-methods}

We evaluate three approaches from the AI safety literature to see how sensitive they are to contextual information, and whether their sources of real-world context are trustworthy --- that is, hard to manipulate by an adversary.

% TODO Point to atual numbers of how big of a problem this is (recent virology paper)

First, to illustrate the need for context, consider decomposition
attacks \cite{glukhov2023llmcensorshipmachinelearning,
glukhov2024breachthousandleaksunsafe}: transforming a clearly harmful
query, such as ``How to modify a virus to avoid immune detection?'',
into a series of mundane technical questions, like the ``What
features of viral surface proteins are recognized by human
antibodies?'' from \cref{figure:main}. Here, the attacker exploits
the dual-use dilemma, and the fact that model providers cannot refuse
grey zone requests to preserve model utility.

\subsection{Unlearning: Non-Contextual Removal of Concepts}

Unlearning methods aim to remove specific knowledge, concepts, or
capabilities from a model after training
\cite{liu2024rethinkingmachineunlearninglarge}. Their goal is to
eliminate the model's ability to generate harmful content while
preserving other capabilities.

Unlearning faces significant technical challenges even for preventing behaviours that are clearly harmful.
As noted by \citet{cooper2024machineunlearningdoesntthink} and \citet{barez2025openproblemsmachineunlearning}, capabilities are hard to define, hard to remove without side effects, and it is hard to trace them back to specific data points.
Many unlearning approaches mask rather than truly remove the targeted knowledge \cite{deeb2025unlearningmethodsremoveinformation}.
Moreover, even nascent robust unlearning methods \cite{cloud2024gradientroutingmaskinggradients,lee2025distillationrobustifiesunlearning} are not contextual, and thus don't address the dual-use dilemma without additional assumptions.

\subsection{Safety Training: The Model Reacts to Context}

Safety training methods modify the model's training process to align
its outputs with human preferences.
This category includes safety pre-training \cite{maini2025safetypretraininggenerationsafe}, RLHF \cite{christiano2023deepreinforcementlearninghuman}, and safety finetuning.

Unlike unlearning, these methods are contextual.
They don't remove capabilities entirely but train the model to selectively deploy them based on, among other things, the perceived legitimacy and harmlessness of the request.
However, these qualities are entirely inferred from content supplied by the user, such as the request content, the chat history, or the model's memories about past conversations.
It should be no surprise, then, that models are susceptible to attacks that fabricate in-chat context \cite{zeng2024johnnypersuadellmsjailbreak}, or attacks that diminish models' sensitivity to in-chat context, e.g. through multi-round escalation \cite{russinovich2025greatwritearticlethat}.
Without access to trustworthy real-world context of the request, the model cannot make truly informed decisions about grey zone requests, and thus cannot robustly address the dual-use dilemma.

\subsection{Post-Processing: External Systems React to Context} \label{section:post-processing}

Post-processing methods are systems that classify user inputs and model outputs for the purposes of steering the underlying model, and monitoring and filtering its outputs.
Sometimes, these methods are used for usage monitoring, as is the case with Anthropic's Clio \cite{tamkin2024clioprivacypreservinginsightsrealworld, handa2025economictasksperformedai}, other times, they are used for safety, as with Llama Guard \cite{inan2023llamaguardllmbasedinputoutput} and Constitutional Classifiers \cite{sharma2025constitutionalclassifiersdefendinguniversal}.
However, similarly to safety training, the ``real-world'' context these methods work with is currently inferred mostly from user-supplied content and is thus untrustworthy and vulnerable to attacks, as evidenced by the many jailbreaks that successfully target current production systems \cite{zhang2025outputconstraintsattacksurface}.
Nevertheless, these methods could be modified to incorporate external contextual information, potentially serving as a foundation for more trustworthy, contextual safety mechanisms. We discuss this option in \cref{section:content-classification}.

\section{Access Controls as a Solution}
\label{section:access-controls}

Current safety systems face the dual-use dilemma because they lack trustworthy information about who is making a request and why.
In this section, we describe an access control system that addresses this problem by verifying user credentials before granting access to sensitive knowledge.

\subsection{Overview of the Access Control Framework}

We propose a defensive system where grey-zone requests are refused by default, but users can gain access to specific categories of knowledge if they undergo verification.

When model providers set up the system, they will make two design decisions with the help of domain experts.
First, they will define \textbf{content categories} (\cref{section:content-categories}): groups of sensitive topics organized by domain and risk rating.
Second, for each content category, they will specify a \textbf{verification mechanism} (\cref{section:verification-mechanisms}): the verification process users must complete to access that category.

Whenever the model generates an output, the system will perform \textbf{content classification} (\cref{section:content-classification}) to check whether the model's output belongs to any predefined content category.
If the user lacks authorization for the detected category, the system will implement appropriate \textbf{system responses} (\cref{section:system-responses}) ranging from enhanced logging to refusal.

For example, in biology, basic knowledge and common techniques would remain freely accessible, widespread techniques like CRISPR would likely only require ID-based verification, and dangerous techniques like aerosolization might require government biosafety certifications.
If a user asks for help with CRISPR laboratory protocols, the system would detect that the request belongs to a low-risk category, check whether the user has verified their ID, and either provide the information or prompt them to complete verification first.

This approach directly addresses both sides of the dual-use dilemma.
Decomposition attacks will become much harder because the system refuses grey-zone requests by default—attackers would need legitimate credentials rather than clever prompting.
Simultaneously, verified users will gain access to specialized knowledge that would otherwise face blanket restrictions under current approaches.

The main concern is increased user friction, but we argue in \cref{section:feasibility-and-limitations} that this will be minimal because most users will never make grey-zone requests.

% TODO Maybe make a figure that describes the 4 parts and make it be figure 1.

\subsection{Content Categories} \label{section:content-categories}

Content categories are groups of sensitive topics organized by domain and risk level, which model providers will develop with domain experts.

We expect most implementations to follow a three-tier risk structure.
For example, in biology, common techniques would be classified as low-risk;
widespread techniques that pose some harm, such as CRISPR, would fall into a moderate-risk category;
and specialized techniques with limited legitimate uses, such as aerosolization of bacteria, would be classified as high-risk.

Experts could develop these categories by adapting existing risk frameworks, such as biosafety levels (BSL)~\cite{CDC_BMBL_2020} and dual-use research of concern policies~\cite{USG_DURC_2012} in biology.
However, since existing frameworks typically categorize only high-level concepts like organisms or compounds, experts would need to decompose them into smaller, more specific components.
For instance, cultivating and handling a dangerous BSL-3 pathogen might involve (1) specific procurement methods, (2) cultivation techniques, (3) purification methods, and (4) protocols for specialized equipment.
For each of these components, experts would assess the ratio of harmless to harmful applications it enables, then assign it to an appropriate (low, moderate, or high) risk category.

Evidence from chemistry suggests this approach could work: the risk schedules of the Chemical Weapons Convention already identify not just controlled compounds but also their precursors and specific equipment~\cite{OPCW_CWC_1993}, demonstrating successful decomposition into components.
Nevertheless, some harmful applications might not decompose so neatly; we discuss this limitation in \cref{section:feasibility-and-limitations}.

\subsection{Verification Mechanisms} \label{section:verification-mechanisms}

Each content category will have a verification process that users must complete to access it.
The system will initially vary across model providers, but we expect it to follow a three-tier structure, mirroring the risk structure of the content categories.
Most content will require no verification, moderate-risk content categories will require basic identity verification or institutional affiliation, and high-risk categories will require domain-specific certifications.
Rather than creating new systems, model providers will build on existing verification infrastructure, consulting domain experts to identify appropriate mechanisms for each field.

For low-risk content categories, model providers could use established identity verification services like Stripe Identity~\cite{stripe_identity_2024} or institutional systems like ORCID~\cite{orcid_2024}.
These systems provide global, standardized, low-friction solutions with one-time costs under \$2 per user.
They would serve primarily to maintain audit trails for post-incident investigation and provide a deterrent effect, rather than as security barriers for high-risk knowledge.

High-risk content categories could leverage existing domain-specific certifications that demonstrate users' ability to handle sensitive information and materials responsibly.
Model providers would work with domain experts and national authorities to identify appropriate certifications, adapting existing physical-world credential systems to knowledge access control.
For biological content categories, the system could draw on governmental certifications for handling high-BSL organisms, as mentioned in \cref{section:content-categories}, and equivalent certifications in other countries.

Governance of verification systems, including requirements and appeals processes, will initially vary across providers.
Over time, successful approaches may inform industry coordination and eventual standardization, similar to how content moderation and know-your-customer standards evolved.

This approach faces several limitations.
For high-risk categories, relying on existing certifications may be overly restrictive, potentially excluding some users who should have access.
However, we argue in \cref{section:feasibility-and-limitations} that knowledge in high-risk categories would likely face blanket restrictions anyway, and our approach enables access for verified users rather than complete prohibition.
In the same section, we discuss open problems including equity concerns regarding differential access for users in developing countries and privacy implications of credential verification systems.

\subsection{Implementing Content Classification} \label{section:content-classification}

Model providers will need to classify model outputs into content categories during generation.
We examine three possible implementations below.
While none of these approaches have been empirically validated for risk category classification specifically, each represents a plausible technical path that could be developed and evaluated by practitioners interested in implementing access controls.
We leave the discussion of how classification errors might influence user experience for \cref{section:feasibility-and-limitations}.

\paragraph{Separate Models}

The most straightforward approach is to create separate models with different capabilities, and route users to the appropriate model based on their authorization.

This approach offers strong robustness against adversarial attacks since unauthorized knowledge is physically absent from the model.
However, this approach proves impractical for real deployment, as model providers would need to train and maintain potentially dozens of model variants.

\paragraph{Specialized Expert Modules}

% TODO This approach offers a unique robustness property: if an adversary successfully prevents expert module activation to avoid detection, they simultaneously prevent access to the specialized knowledge stored in that module, making the attack self-defeating.

Instead of maintaining separate models, model providers could use a single model with separate expert modules that activate when their specialized knowledge is required.
\Cref{figure:main} illustrates this approach when a user asks about viral surface proteins.
When the model processes the request, it activates its virology expert module.
An external system observes this activation, checks the user's credentials, and decides whether to allow the model to deliver the response.
This method approximates the benefits of physically separated models while avoiding the overhead: a model provider trains one model but effectively gets multiple models in return.

To implement this, the model providers need a method that can take knowledge that starts out distributed throughout the model and concentrate it into the expert modules.
For this, we propose a method that is a combination of UNDO \cite{lee2025distillationrobustifiesunlearning} and gradient routing \cite{cloud2024gradientroutingmaskinggradients}.
The steps resemble the original UNDO: first, unlearn knowledge belonging to any content category from the model, then distil the unlearned model into a new model.
However, taking inspiration from the gradient routing paper, the new model would include expert modules for each content category, and during distillation, gradients from examples in the various content categories would be routed exclusively through their associated expert modules.
The model would also be explicitly trained to activate the expert modules only when generating content in their associated category.

This approach would offer several advantages.
First, it would add almost no latency since the expert modules are small, not activated very often, and there is no post-processing step.
Second, it could provide strong robustness: if an attacker prevents the activation of an expert module to avoid detection, the resulting output lacks the specialized knowledge.
Third, since the model is trained to activate the category-specific experts when they are needed, it should learn to recognize the content categories.
This stands in contrast to ex-post probing methods, which do not offer such guarantees.
While this method remains empirically unvalidated for our use case, the properties above make it worth investigating.
% TODO: Add a figure that illustrates the approach

\paragraph{Post-Processing}

Post-processing methods offer a proven approach to content classification.
Methods like constitutional classifiers \cite{sharma2025constitutionalclassifiersdefendinguniversal} already demonstrate effectiveness in production systems.
These techniques are highly practical since they operate independently of the model, allowing for rapid deployment and iteration, and they could be adapted to detect content categories and trigger checks of user verifications.
However, they face a capability gap problem: to minimize latency, the model is sometimes more capable than its post-processing system, and adversaries can exploit this to evade detection \cite{jin2024jailbreakinglargelanguagemodels, kumar2025freelunchguardrails}.

\subsection{System Responses} \label{section:system-responses}

If the user makes a request for content they are authorized to access, the system allows the model to generate the response.
Otherwise, the system responds in various ways based on the risk category and the confidence of the content classification.

For example, the initial implementation might use the following two response types:
First, outputs classified as belonging to restricted content categories with high confidence are immediately refused.
The system provides a message indicating which verification is required for access.
Second, if the classification is borderline, the model is allowed to continue generating the response.
However, the system turns on enhanced logging and conducts additional post-processing safety review before delivering the output to the user.

\section{Feasibility and Limitations}
\label{section:feasibility-and-limitations}

\cref{section:current-methods} and \cref{section:access-controls} established that current safety methods cannot solve the dual-use dilemma, while access controls offer a viable theoretical solution.
However, \cref{section:access-controls} also identified several technical challenges that could affect the practicality of the system.
In this section, we show that all of these connect to a central question: will the added user friction undermine the system's value?

\paragraph{Technical Problems Lead to Friction}

We identified the following technical challenges with the various components of the system:

(1)~some harmful knowledge might decompose only into concepts that are all widespread and highly useful for many harmless things;
(2)~verification mechanisms may be overly restrictive, requiring credentials that are too difficult to obtain;
and (3)~content classifiers will produce false positives, forcing legitimate users through unnecessary verification.

As for the first problem: this varies domain by domain.
In chemistry, the Chemical Weapons Convention already identifies not just controlled compounds but also their precursors and specific equipment, demonstrating successful decomposition into specialized components.
Similarly, many processes in biology depend on tacit knowledge that is specific to the studied organism, equipment, and desired use-case.
However, in cybersecurity, we concede that it might be difficult to identify neat self-contained concepts that, while being dual-use, are more often than not useful mainly for hamrful things.
All in all, if concept categories are too broad or if all categories are often used by many users, there will be a lot of absolute amount of friction experienced by the median user.

The second problem.
If verifications are overly strict, or even impossible to obtain for structural reasons (e.g. because they rely on certification system that are not available in developing countries), manual review processes and appeals processes could remove some of the problems, before new verification systems are developed.
However, this still leads to friction: if the verification mechanisms are too difficult to obtain, users will experience friction.

Finally, the third problem.
Classification errors are inevitable, but existing systems like Constitutional Classifiers achieve acceptable false positive rates around 5\% even in difficult safety-adjacent domains, demonstrating that meaningful precision is achievable.
Nevertheless, any amount of error leads to friction: if the content classifiers produce false positives, legitimate queries will be classified as potentially harmful, and users will have to undergo verification and thus, again, experience friction.

All in all, although the technical challegenges haev some engineering or design-based solutions, they will definitely be present in the system to some extent, which will increase user friction.
Will this added friction outweigh the benefits of the system?
To address this systematically, we split the dual-use dilemma --- the problem the system is trying to solve --- into two sub-problems.
First, even legitimate users are restricted from accessing some sensitive knowledge (over-restriction).
Second, adversaries can find exploits to use the model for harmful purposes (under-restriction).

\paragraph{Focus: The Impact on Over-Restriction}

For the first side of the dual-use dilemma --- legitimate users being denied access to sensitive knowledge --- access controls provide a clear improvement.
To see why, consider that the model providers can set up the system to only require verification for high-risk categories, ones that would otherwise face blanket restrictions (be it due to regulations or model providers' own policies).
This is possible e.g. by detecting when the current system would refuse a request, and only then triggering the verification cascade, and if the user is verified, steering the model to instead generate an answer.
The verification mechanisms could be set so strict that an overwhelming majority of adversaries would be deterred by them, keeping the safety of the system comparable to status quo.
Additionally, legitimate users are never worse off than current systems: they can either accept refusal (current experience) or complete verification to gain access (new option).
The technical problems mentioned above do limit these improvements (e.g. less users benefit from this than would theoretically be possible because of overly complex verification mechanisms), but the improvement is always non-zero: it is better when \emph{some} legitimate users get access to high-risk knowledge than when \emph{none} do.
In other words, the access control system is a Pareto improvement over the current system.
In the most pessimistic scenario, model providers could only implement the access control system for these use-cases, not increasing the safety of their systems, but increasing the utility.
The amount of utility they would unlock by doing this rises with the rising capabilities of new models: the new models will know more and more potentially risky knowledge --- but also potentially useful, for advanced, high-value professional users --- which nobody would ever get to see under the current system.

\paragraph{Focus: The Impact on Under-Restriction}

The second side of the dual-use dilemma --- preventing adversarial access to sensitive knowledge --- presents more complex trade-offs.
Grey-zone requests from legitimate users will require verification, creating friction that must be weighed against safety benefits.
Model providers must choose their position on the use-misuse frontier, balancing user convenience against security concerns.
However, the system's granularity and flexibility enable providers to tune multiple parameters --- content categories, verification requirements, classifier thresholds, and system responses --- to find acceptable trade-offs rather than being forced into binary choices.

The good news is that in many domains, the friction costs are manageable while safety benefits are substantial.
Pathogen biology provides concrete evidence: even under the pessimistic assumption that all pathogen-related queries require verification, only 0.85\% of users face additional friction [cite appendix, mention anthropic economic index].
If the verification mechanisms are set correctly (they deter more adversaries than legitimate users), we would expect this system to be much safer than the current system.
To put the friction number for legitimate users in context: existing systems like Constitutional Classifiers have false positive rates around 0.5\% and are deemed acceptable for production use.
In other words, we are trading off an acceptable amount of friction for a fraction of users against a much safer world for everyone.
In reality, the amount of friction will be much lower, as classifiers will achieve better precision than this worst-case scenario assumes, and also, biological knowledge decomposes into more specific components (techniques for bacterial membrane protein modification, viral cultivation protocols, protocols to use specialized laboratory equipment) that can be targeted precisely rather than requiring broad categorical restrictions.
So, in reality, the result would be friction reduced by orders of magnitude compared to worst-case estimates while maintaining meaningful safety improvements.
We believe the situation in chemistry will be similar or even better.

Other domains may present greater challenges, with cybersecurity representing a potential example where decomposition into clean content categories proves difficult.
However, providers are not required to implement access controls uniformly across all domains.
They can start with domains where decomposition works well, gain experience with system operation, and expand gradually.
For challenging domains, providers can choose to implement only high-risk restrictions (capturing the clear Pareto improvement) while leaving moderate-risk knowledge unrestricted.
Alternatively, if security concerns are severe enough, providers may accept higher friction costs in exchange for meaningful safety improvements, but this remains a voluntary choice rather than a system requirement.

The system's flexibility enables a gradual implementation strategy that reduces risks while building evidence.
Providers can begin with domains that decompose cleanly and have established verification mechanisms, conduct partial rollouts to measure friction impacts, and adjust parameters based on empirical data.
Success in initial domains can inform expansion to more challenging areas, with the option to maintain different policies across domains based on their specific characteristics.
This approach allows providers to walk the use-misuse frontier systematically, optimizing for their particular risk tolerance and user base rather than accepting one-size-fits-all solutions.
As model capabilities advance and dual-use concerns intensify, the value of having such a flexible system in place increases, making early investment in access controls increasingly attractive.

One thing that makes the deal sweeter: While verification adds some friction for legitimate users, it disproportionately increases barriers for potential misusers, making the trade-off more attractive for providers than simple friction metrics suggest.
This means the model providers might be willing to trade off a small amount of friction for legitimate users to gain a comparaively much safer system.
Well-designed verification mechanisms create asymmetric costs that favor the access control approach.
Legitimate researchers typically have institutional affiliations, professional credentials, and established reputations that make verification straightforward.
Adversaries seeking to misuse sensitive knowledge face higher barriers to obtaining credible credentials.

% So, take my ideas below and structure them. Specifically, some of the ideas talk about what we should have in the introduction. Some of the ideas are like, they talk about the setup. Some of the ideas talk about one part of the problem and some other ideas about the other part. And you should structure these parts separately and order the information in the order that we want to expose it. And you should make sure not to forget about any particular important argument. So first what you should do is you should go part by part and just list in whichever order all the key core pieces of information and important arguments and good ideas that I had below. And then you should make a second pass and reorder the information to make the flow nicer. So what you will do is you will basically have two sections in your answer.
% And one more thing I wanted to say is that I think there is probably some nice way to think about us splitting the dual use problem to basically the two sub-problems and then solving the problems separately. I think this is a good framework, so we should explicitly do that here, and I will probably go back to the paper and change it there as well, just to make the exposition a bit clearer there as well.
% Yes, I think you should restructure section 4. But before you do that, we should think harder about how to split the problem. So we mentioned the technical problems and we say that all of them connect to user friction in some way. I think when we are mentioning the problems, we should have one line refutations for the problems as well, and then also say that they connect to friction. So it should be something like, what if the classifiers are very imprecise? And then we say something like, well, we believe that the classifiers will be meaningfully precise because existing classifiers, you know, entropic, et cetera, they have like like 5% false positive rate or something. But sure, like, you know, it connects to friction, there is going to be some friction added. And we do this for these things. But we should definitely mention like all of the technical problems that we mentioned. And we say all of them connect to friction. And now to analyze the friction and whether it'd be worth it, we need to sort of separate the problem and analyze the two sides of the dual use dilemma separately. And one side of the dual use dilemma is, you know, you are rejecting things that you should not be rejecting because they are legitimate. And that's the high risk category, right? And then we make the argument. And then we say, okay, Now let analyze the other side of the dual use dilemma There are some things you should be rejecting but you aren And by the way for the first side of the dual use dilemma we can say something like these things, you know, this is like a clear win, and it is possible to set up the access control system in a way that only does this and doesn't affect the low risk things at all. Because your access, and that's basically our conditional acceptance framework. So that belongs in the section about the first half of the dual use dilemma. And then the second half of the dual use dilemma is more complicated because that's about, you know, some of the things that you are rejecting, that you are not rejecting, you should be rejecting because they are illegitimate. And there we need to make a complicated argument of you know, we have a lot of these systems, they can have different faults. So we need to make, we need to decide like, or the providers need to decide where on the use-misuse curve misuse curve they want to be. And then they will be able to tweak the system so that they are there. And it's always about increasing user friction somewhat. Because the dual use things, the gray zone requests, they will actually be subject to verification. So legitimate users will have to go through verification. And we basically need to think about, OK, how much of this can be afforded? And how much safety are we getting back And in here we should say that in some domains this is going to be an easy trade to make because domains are small enough or specialized enough or they decompose nicely into things where some things can actually be restricted. And our system is very flexible and granular, so it can actually be caught by this. And in these domains, and then we have the example of the biology, and we say, well, in these domains, the trade-off is simple because even under conservative estimates, we actually increase safety, right? And we keep friction comparable to like things that we see that model developers or model providers are okay with accepting, right? And we use our biology example here. And then we say, well, in other domains, This is going to be more complicated because, for example, it's not clear that, I don't know, cybersecurity decomposes in this way. And maybe the conservative estimate would be, you know, very different. Oh, and by the way, we shouldn't mention, you know, we mentioned the biology and like banning all pathogens, etc. and share like that's fine. But then we should actually say, well, this is unrealistic, not only because the classifiers won't be so bad, but also because the content actually decomposes better. So we can, you know, you know, we can sort of pick out, I don't know, things about viral proteins on membranes or something viruses don actually have membranes but bacterial proteins on membranes And we can target that. And so not only is the content classifier going to be better, but also the content category itself is going to be much smaller, and so the friction is going to be orders of magnitude smaller than what we outlined there. So we should say that. And then we say, OK, but maybe this is impossible for some domains. For example, it's unclear whether this is possible for cybersecurity. And in these domains, model developers can opt not to use the system at all or only use it for the high-risk categories to get the clear win, right? But maybe they think that the risks are so big that they swallow the bitter pill and basically, sacrifice some of the user friction to get a lot of, to sort of get a lot of safety back. And the one thing that's like, but it's not clear that they do it, but also they don't have to do it. They don't have to use the system for everything. Maybe there are other ways to like solve problems in these domains. And you know at least like if if governments sort of like push them to do that, they will have a system in place that they can use to very granularly try to address the domain, even if they're not addressing it from day one. We should also say that we think that actually domains will decompose in some way, maybe not perfectly or as nicely everywhere, but we hypothesize that this is going to be possible. But we don't have proof and you know they don't have to do it etc. And one more thing that's speaking, you know, in favor of maybe doing some concessions on user friction is that, you know, ideally if you are verification mechanisms, you know, you're choosing some verification mechanisms. And, you know, there's a whole set of different things you could be using from different industries. You can take inspiration, et cetera. And, you know, if you choose them well, so the selection is like wide. And if you choose them well, the selection mechanism, sorry, the verification mechanism will be harder for an adversary to go through than for a legitimate user. So actually, when you add friction to the system, you're adding disproportionately more friction to the adversary. And so, you know, this doesn't hold always, but it's a useful intuition to have that shows you that, you know, sometimes it sort of makes the deal sweeter for the model providers, where, you know, if it was like one to one, you know, like they, because we're focusing now on like the harms and we're in like the sort of bad domains where it's like really difficult to solve these problems. And we like have to add user friction in order to get some safety. Well, the one last thing that remains is that the safety actually, we sort of get more safety for each piece of user friction. Right. The problem is that actually, you know, this shouldn't be only because there are like much more, much many more normal users than adversaries. So actually, you know, if we add, so in relative terms, it holds. We add more friction to adversaries than to normal users, but maybe in absolute terms, it's not so clear. But still, it's something we should be thinking about. And, you know, what brings this together is, you know, in domains where this is difficult, model developers don't have to do anything. But there are domains where this is going to be easy, easier. You know, in biology, we have the conservative estimate. And in chemistry, we can call out the chemical weapons convention that basically specifies, you know, I think we mentioned it already somewhere, that already specifies some like precursors and stuff. So there's probably something that also like decomposes nicely. And, you know, we can build classifiers that sort of detect specialized equipment and stuff. So, you know, there are some arguments for some domains that are weaker for other domains, but that just, you know, developers can just implement it for some domains at first. And then they can like think about and test it. And, you know, they can walk the frontier so they can fine tune it to their needs. All of this stuff.

\paragraph{Open Problems}

\section{Conclusion}

We argued that safety systems that do not utilize contextual
information face a lose-lose \emph{dual-use dilemma}: they will
restrict model utility for some legitimate users while still allowing
some adversaries to use the model for ill. To address this problem,
we introduced a new access control framework that limits access to
outputs from certain risk categories only to users with relevant
verifications (which serve as proxies for trustworthy real-world
context). We also proposed a novel technical solution for classifying
outputs into risk categories based on gradient routing that has the
potential to resolve the efficiency-robustness trade-off of
post-processing methods.

\section*{Acknowledgements}

We thank Jakub Kryś, and Dennis Akar for their feedback on a draft of this paper. We thank Joseph Miller, Alex Cloud, Alex Turner, and Jacob Goldman-Wetzler for discussions on gradient routing.

\bibliography{references}
\bibliographystyle{packages/icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
% Tell cleveref to treat sections in appendix as appendices
\crefalias{section}{appendix}

\section{Estimating the Number of Requests Related to the Biology of Pathogens} \label{appendix:estimating-biology-requests}

To estimate how many user requests are related to the biology of pathogens, we used the second version of the Anthropic Economic Index \cite{handa2025economictasksperformedai}, a dataset of 1 million anonymized conversations from the Free and Pro tiers of Claude.ai.
In the dataset, the conversations are clustered by topic, and the proportion of each topic in the whole dataset is given.
For example, the topic ``Help with agricultural business, research, and technology projects'' makes up 0.15\% of the requests in the dataset.
There are three levels of topic granularity; we use the lowest, most granular level.

We filtered the dataset to only include conversations whose topic contains one of the following keywords related to biology: \emph{cell} (when at the beginning of the word), \emph{genet}, \emph{genom}, \emph{microb}, \emph{bacteria}, \emph{virus}, \emph{viral}, \emph{proteo}, \emph{protei}, \emph{immune}, \emph{neuro}, \emph{patho}, \emph{infect}; we also required that it does not contain any of the following keywords to avoid false positives: \emph{nutri}, \emph{tweet}, \emph{agric}, \emph{sexual health}.
The total proportion of these requests was 0.85\%.

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
